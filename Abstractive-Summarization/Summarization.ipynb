{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstractive Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on [Seq2seq learning](https://arxiv.org/abs/1409.3215)\n",
    "with [attention mechanism](https://arxiv.org/abs/1409.0473), specifically [local attention](https://nlp.stanford.edu/pubs/emnlp15_attn.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Pre-processed Dataset\n",
    "\n",
    "The Data is preprocessed in [Data_Pre-Processing.ipynb](https://github.com/JRC1995/Abstractive-Summarization/blob/master/Data_Pre-Processing.ipynb)\n",
    "\n",
    "Dataset source: https://www.kaggle.com/snap/amazon-fine-food-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('Processed_Data/Amazon_Reviews_Processed.json') as file:\n",
    "\n",
    "    for json_data in file:\n",
    "        saved_data = json.loads(json_data)\n",
    "\n",
    "        vocab2idx = saved_data[\"vocab\"]\n",
    "        embd = saved_data[\"embd\"]\n",
    "        train_batches_text = saved_data[\"train_batches_text\"]\n",
    "        test_batches_text = saved_data[\"test_batches_text\"]\n",
    "        val_batches_text = saved_data[\"val_batches_text\"]\n",
    "        train_batches_summary = saved_data[\"train_batches_summary\"]\n",
    "        test_batches_summary = saved_data[\"test_batches_summary\"]\n",
    "        val_batches_summary = saved_data[\"val_batches_summary\"]\n",
    "        train_batches_true_text_len = saved_data[\"train_batches_true_text_len\"]\n",
    "        val_batches_true_text_len = saved_data[\"val_batches_true_text_len\"]\n",
    "        test_batches_true_text_len = saved_data[\"test_batches_true_text_len\"]\n",
    "        train_batches_true_summary_len = saved_data[\"train_batches_true_summary_len\"]\n",
    "        val_batches_true_summary_len = saved_data[\"val_batches_true_summary_len\"]\n",
    "        test_batches_true_summary_len = saved_data[\"test_batches_true_summary_len\"]\n",
    "\n",
    "        break\n",
    "        \n",
    "idx2vocab = {v:k for k,v in vocab2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 300\n",
    "learning_rate = 0.001\n",
    "epochs = 5\n",
    "max_summary_len = 31 # should be summary_max_len as used in data_preprocessing with +1 (+1 for <EOS>) \n",
    "D = 5 # D determines local attention window size\n",
    "window_len = 2*D+1\n",
    "l2=1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/luispreciado/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf \n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "embd_dim = len(embd[0])\n",
    "\n",
    "tf_text = tf.placeholder(tf.int32, [None, None])\n",
    "tf_embd = tf.placeholder(tf.float32, [len(vocab2idx),embd_dim])\n",
    "tf_true_summary_len = tf.placeholder(tf.int32, [None])\n",
    "tf_summary = tf.placeholder(tf.int32,[None, None])\n",
    "tf_train = tf.placeholder(tf.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(x,rate,training):\n",
    "    return tf.cond(tf_train,\n",
    "                    lambda: tf.nn.dropout(x,rate=0.3),\n",
    "                    lambda: x)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed vectorized text\n",
    "\n",
    "Dropout used for regularization \n",
    "(https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embd_text = tf.nn.embedding_lookup(tf_embd, tf_text)\n",
    "\n",
    "embd_text = dropout(embd_text,rate=0.3,training=tf_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM function\n",
    "\n",
    "More info: \n",
    "<br>\n",
    "https://dl.acm.org/citation.cfm?id=1246450, \n",
    "<br>\n",
    "https://www.bioinf.jku.at/publications/older/2604.pdf,\n",
    "<br>\n",
    "https://en.wikipedia.org/wiki/Long_short-term_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM(x,hidden_state,cell,input_dim,hidden_size,scope):\n",
    "    \n",
    "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):\n",
    "        \n",
    "        w = tf.get_variable(\"w\", shape=[4,input_dim,hidden_size],\n",
    "                                    dtype=tf.float32,\n",
    "                                    trainable=True,\n",
    "                                    initializer=tf.glorot_uniform_initializer())\n",
    "        \n",
    "        u = tf.get_variable(\"u\", shape=[4,hidden_size,hidden_size],\n",
    "                            dtype=tf.float32,\n",
    "                            trainable=True,\n",
    "                            initializer=tf.glorot_uniform_initializer())\n",
    "        \n",
    "        b = tf.get_variable(\"bias\", shape=[4,1,hidden_size],\n",
    "                    dtype=tf.float32,\n",
    "                    trainable=True,\n",
    "                    initializer=tf.zeros_initializer())\n",
    "        \n",
    "    input_gate = tf.nn.sigmoid( tf.matmul(x,w[0]) + tf.matmul(hidden_state,u[0]) + b[0])\n",
    "    forget_gate = tf.nn.sigmoid( tf.matmul(x,w[1]) + tf.matmul(hidden_state,u[1]) + b[1])\n",
    "    output_gate = tf.nn.sigmoid( tf.matmul(x,w[2]) + tf.matmul(hidden_state,u[2]) + b[2])\n",
    "    cell_ = tf.nn.tanh( tf.matmul(x,w[3]) + tf.matmul(hidden_state,u[3]) + b[3])\n",
    "    cell = forget_gate*cell + input_gate*cell_\n",
    "    hidden_state = output_gate*tf.tanh(cell)\n",
    "    \n",
    "    return hidden_state, cell\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-Directional LSTM Encoder\n",
    "\n",
    "(https://maxwell.ict.griffith.edu.au/spl/publications/papers/ieeesp97_schuster.pdf)\n",
    "\n",
    "More Info: https://machinelearningmastery.com/develop-bidirectional-lstm-sequence-classification-python-keras/\n",
    "\n",
    "Bi-directional LSTM encoder has a forward encoder and a backward encoder. The forward encoder encodes a text sequence from start to end, and the backward encoder encodes the text sequence from end to start.\n",
    "The final output is a combination (in this case, a concatenation) of the forward encoded text and the backward encoded text\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = tf.shape(embd_text)[1] #text sequence length\n",
    "N = tf.shape(embd_text)[0] #batch_size\n",
    "\n",
    "i=0\n",
    "hidden=tf.zeros([N, hidden_size], dtype=tf.float32)\n",
    "cell=tf.zeros([N, hidden_size], dtype=tf.float32)\n",
    "hidden_forward=tf.TensorArray(size=S, dtype=tf.float32)\n",
    "\n",
    "#shape of embd_text: [N,S,embd_dim]\n",
    "embd_text_t = tf.transpose(embd_text,[1,0,2]) \n",
    "#current shape of embd_text: [S,N,embd_dim]\n",
    "\n",
    "def cond(i, hidden, cell, hidden_forward):\n",
    "    return i < S\n",
    "\n",
    "def body(i, hidden, cell, hidden_forward):\n",
    "    x = embd_text_t[i]\n",
    "    \n",
    "    hidden,cell = LSTM(x,hidden,cell,embd_dim,hidden_size,scope=\"forward_encoder\")\n",
    "    hidden_forward = hidden_forward.write(i, hidden)\n",
    "\n",
    "    return i+1, hidden, cell, hidden_forward\n",
    "\n",
    "_, _, _, hidden_forward = tf.while_loop(cond, body, [i, hidden, cell, hidden_forward])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=S-1\n",
    "hidden=tf.zeros([N, hidden_size], dtype=tf.float32)\n",
    "cell=tf.zeros([N, hidden_size], dtype=tf.float32)\n",
    "hidden_backward=tf.TensorArray(size=S, dtype=tf.float32)\n",
    "\n",
    "def cond(i, hidden, cell, hidden_backward):\n",
    "    return i >= 0\n",
    "\n",
    "def body(i, hidden, cell, hidden_backward):\n",
    "    x = embd_text_t[i]\n",
    "    hidden,cell = LSTM(x,hidden,cell,embd_dim,hidden_size,scope=\"backward_encoder\")\n",
    "    hidden_backward = hidden_backward.write(i, hidden)\n",
    "\n",
    "    return i-1, hidden, cell, hidden_backward\n",
    "\n",
    "_, _, _, hidden_backward = tf.while_loop(cond, body, [i, hidden, cell, hidden_backward])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Forward and Backward Encoder Hidden States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_forward = hidden_forward.stack()\n",
    "hidden_backward = hidden_backward.stack()\n",
    "\n",
    "encoder_states = tf.concat([hidden_forward,hidden_backward],axis=-1)\n",
    "encoder_states = tf.transpose(encoder_states,[1,0,2])\n",
    "\n",
    "encoder_states = dropout(encoder_states,rate=0.3,training=tf_train)\n",
    "\n",
    "final_encoded_state = dropout(tf.concat([hidden_forward[-1],hidden_backward[-1]],axis=-1),rate=0.3,training=tf_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of attention scoring function\n",
    "\n",
    "Given a sequence of encoder states ($H_s$) and the decoder hidden state ($H_t$) of current timestep $t$, the equation for computing attention score is:\n",
    "\n",
    "$$Score = (H_s.W_a).H_t^T $$\n",
    "\n",
    "($W_a$ = trainable parameters)\n",
    "\n",
    "(https://nlp.stanford.edu/pubs/emnlp15_attn.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_score(encoder_states,decoder_hidden_state,scope=\"attention_score\"):\n",
    "    \n",
    "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):\n",
    "        Wa = tf.get_variable(\"Wa\", shape=[2*hidden_size,2*hidden_size],\n",
    "                                    dtype=tf.float32,\n",
    "                                    trainable=True,\n",
    "                                    initializer=tf.glorot_uniform_initializer())\n",
    "        \n",
    "    encoder_states = tf.reshape(encoder_states,[N*S,2*hidden_size])\n",
    "    \n",
    "    encoder_states = tf.reshape(tf.matmul(encoder_states,Wa),[N,S,2*hidden_size])\n",
    "    decoder_hidden_state = tf.reshape(decoder_hidden_state,[N,2*hidden_size,1])\n",
    "    \n",
    "    return tf.reshape(tf.matmul(encoder_states,decoder_hidden_state),[N,S])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Attention Function\n",
    "\n",
    "Based on: https://nlp.stanford.edu/pubs/emnlp15_attn.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def align(encoder_states, decoder_hidden_state,scope=\"attention\"):\n",
    "    \n",
    "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):\n",
    "        Wp = tf.get_variable(\"Wp\", shape=[2*hidden_size,128],\n",
    "                                    dtype=tf.float32,\n",
    "                                    trainable=True,\n",
    "                                    initializer=tf.glorot_uniform_initializer())\n",
    "        \n",
    "        Vp = tf.get_variable(\"Vp\", shape=[128,1],\n",
    "                            dtype=tf.float32,\n",
    "                            trainable=True,\n",
    "                            initializer=tf.glorot_uniform_initializer())\n",
    "    \n",
    "    positions = tf.cast(S-window_len,dtype=tf.float32) # Maximum valid attention window starting position\n",
    "    \n",
    "    # Predict attention window starting position \n",
    "    ps = positions*tf.nn.sigmoid(tf.matmul(tf.tanh(tf.matmul(decoder_hidden_state,Wp)),Vp))\n",
    "    # ps = (soft-)predicted starting position of attention window\n",
    "    pt = ps+D # pt = center of attention window where the whole window length is 2*D+1\n",
    "    pt = tf.reshape(pt,[N])\n",
    "    \n",
    "    i = 0\n",
    "    gaussian_position_based_scores = tf.TensorArray(size=S,dtype=tf.float32)\n",
    "    sigma = tf.constant(D/2,dtype=tf.float32)\n",
    "    \n",
    "    def cond(i,gaussian_position_based_scores):\n",
    "        \n",
    "        return i < S\n",
    "                      \n",
    "    def body(i,gaussian_position_based_scores):\n",
    "        \n",
    "        score = tf.exp(-((tf.square(tf.cast(i,tf.float32)-pt))/(2*tf.square(sigma)))) \n",
    "        # (equation (10) in https://nlp.stanford.edu/pubs/emnlp15_attn.pdf)\n",
    "        gaussian_position_based_scores = gaussian_position_based_scores.write(i,score)\n",
    "            \n",
    "        return i+1,gaussian_position_based_scores\n",
    "                      \n",
    "    i,gaussian_position_based_scores = tf.while_loop(cond,body,[i,gaussian_position_based_scores])\n",
    "    \n",
    "    gaussian_position_based_scores = gaussian_position_based_scores.stack()\n",
    "    gaussian_position_based_scores = tf.transpose(gaussian_position_based_scores,[1,0])\n",
    "    gaussian_position_based_scores = tf.reshape(gaussian_position_based_scores,[N,S])\n",
    "    \n",
    "    scores = attention_score(encoder_states,decoder_hidden_state)*gaussian_position_based_scores\n",
    "    scores = tf.nn.softmax(scores,axis=-1)\n",
    "    \n",
    "    return tf.reshape(scores,[N,S,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Decoder With Local Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"decoder\",reuse=tf.AUTO_REUSE):\n",
    "    SOS = tf.get_variable(\"sos\", shape=[1,embd_dim],\n",
    "                                dtype=tf.float32,\n",
    "                                trainable=True,\n",
    "                                initializer=tf.glorot_uniform_initializer())\n",
    "    \n",
    "    # SOS represents starting marker \n",
    "    # It tells the decoder that it is about to decode the first word of the output\n",
    "    # I have set SOS as a trainable parameter\n",
    "    \n",
    "    Wc = tf.get_variable(\"Wc\", shape=[4*hidden_size,embd_dim],\n",
    "                            dtype=tf.float32,\n",
    "                            trainable=True,\n",
    "                            initializer=tf.glorot_uniform_initializer())\n",
    "    \n",
    "\n",
    "\n",
    "SOS = tf.tile(SOS,[N,1]) #now SOS shape: [N,embd_dim]\n",
    "inp = SOS\n",
    "hidden=final_encoded_state\n",
    "cell=tf.zeros([N, 2*hidden_size], dtype=tf.float32)\n",
    "decoder_outputs=tf.TensorArray(size=max_summary_len, dtype=tf.float32)\n",
    "outputs=tf.TensorArray(size=max_summary_len, dtype=tf.int32)\n",
    "\n",
    "attention_scores = align(encoder_states,hidden)\n",
    "encoder_context_vector = tf.reduce_sum(encoder_states*attention_scores,axis=1)\n",
    "\n",
    "for i in range(max_summary_len):\n",
    "    \n",
    "    inp = dropout(inp,rate=0.3,training=tf_train)\n",
    "    \n",
    "    inp = tf.concat([inp,encoder_context_vector],axis=-1)\n",
    "    \n",
    "    hidden,cell = LSTM(inp,hidden,cell,embd_dim+2*hidden_size,2*hidden_size,scope=\"decoder\")\n",
    "    \n",
    "    hidden = dropout(hidden,rate=0.3,training=tf_train)\n",
    "    \n",
    "    attention_scores = align(encoder_states,hidden)\n",
    "    encoder_context_vector = tf.reduce_sum(encoder_states*attention_scores,axis=1)\n",
    "    \n",
    "    concated = tf.concat([hidden,encoder_context_vector],axis=-1)\n",
    "    \n",
    "    linear_out = tf.nn.tanh(tf.matmul(concated,Wc))\n",
    "    decoder_output = tf.matmul(linear_out,tf.transpose(tf_embd,[1,0])) \n",
    "    # produce unnormalized probability distribution over vocabulary\n",
    "    \n",
    "    \n",
    "    decoder_outputs = decoder_outputs.write(i,decoder_output)\n",
    "    \n",
    "    # Pick out most probable vocab indices based on the unnormalized probability distribution\n",
    "    \n",
    "    next_word_vec = tf.cast(tf.argmax(decoder_output,1),tf.int32)\n",
    "\n",
    "    next_word_vec = tf.reshape(next_word_vec, [N])\n",
    "\n",
    "    outputs = outputs.write(i,next_word_vec)\n",
    "\n",
    "    next_word = tf.nn.embedding_lookup(tf_embd, next_word_vec)\n",
    "    inp = tf.reshape(next_word, [N, embd_dim])\n",
    "    \n",
    "    \n",
    "decoder_outputs = decoder_outputs.stack()\n",
    "outputs = outputs.stack()\n",
    "\n",
    "decoder_outputs = tf.transpose(decoder_outputs,[1,0,2])\n",
    "outputs = tf.transpose(outputs,[1,0])\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Cross Entropy Cost Function and L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_trainables = [var for var in tf.trainable_variables() if\n",
    "                       not(\"Bias\" in var.name or \"bias\" in var.name\n",
    "                           or \"noreg\" in var.name)]\n",
    "\n",
    "regularization = tf.reduce_sum([tf.nn.l2_loss(var) for var\n",
    "                                in filtered_trainables])\n",
    "\n",
    "with tf.variable_scope(\"loss\"):\n",
    "\n",
    "    epsilon = tf.constant(1e-9, tf.float32)\n",
    "\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=tf_summary, logits=decoder_outputs)\n",
    "\n",
    "    pad_mask = tf.sequence_mask(tf_true_summary_len,\n",
    "                                maxlen=max_summary_len,\n",
    "                                dtype=tf.float32)\n",
    "\n",
    "    masked_cross_entropy = cross_entropy*pad_mask\n",
    "\n",
    "    cost = tf.reduce_mean(masked_cross_entropy) + \\\n",
    "        l2*regularization\n",
    "\n",
    "    cross_entropy = tf.reduce_mean(masked_cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing predicted sequence with labels\n",
    "comparison = tf.cast(tf.equal(outputs, tf_summary),\n",
    "                     tf.float32)\n",
    "\n",
    "# Masking to ignore the effect of pads while calculating accuracy\n",
    "pad_mask = tf.sequence_mask(tf_true_summary_len,\n",
    "                            maxlen=max_summary_len,\n",
    "                            dtype=tf.bool)\n",
    "\n",
    "masked_comparison = tf.boolean_mask(comparison, pad_mask)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = tf.reduce_mean(masked_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "gvs = optimizer.compute_gradients(cost, all_vars)\n",
    "\n",
    "capped_gvs = [(tf.clip_by_norm(grad, 5), var) for grad, var in gvs] # Gradient Clipping\n",
    "\n",
    "train_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load checkpoint? y/n: n\n",
      "\n",
      "\n",
      "\n",
      "STARTING TRAINING\n",
      "\n",
      "\n",
      "Iter 0, Cost= 1.493, Acc = 0.00%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "yesterday , i bought one pack of 24 oz the exact same item at a local whole foods store at $ 6.99 . i do n't want to buy the pack of 4 at a higher unit price here .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "sweetened yah cashew barrels sweetened sweetened bushel grammy cocoa sweetened yah 7-9 corn sweetened sweetened yah chopped sweetened cocoa yah soybeans yah soybeans sweetened sweetened yah cocoa yah sweetened sweetened bushel \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "not a great price \n",
      "\n",
      "\n",
      "Iter 100, Cost= 0.761, Acc = 27.27%\n",
      "Iter 200, Cost= 0.817, Acc = 25.36%\n",
      "Iter 300, Cost= 0.636, Acc = 29.13%\n",
      "Iter 400, Cost= 0.804, Acc = 26.62%\n",
      "Iter 500, Cost= 0.852, Acc = 21.09%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "i got the wrong item . i paid $ 20 and i got an item worth $ 5 . they refunded me the money\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "i \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "sent me the wrong product \n",
      "\n",
      "\n",
      "Iter 600, Cost= 0.735, Acc = 30.23%\n",
      "Iter 700, Cost= 0.708, Acc = 23.08%\n",
      "Iter 800, Cost= 0.879, Acc = 22.08%\n",
      "Iter 900, Cost= 0.633, Acc = 31.58%\n",
      "Iter 1000, Cost= 0.842, Acc = 22.64%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "i am so sick and tired of natures variety always is making you think your getting the one protein you want then .... it has to ruin it everytime adding another dam protein i wanted . venison <UNK> just venison cause she 's allergic to beef lamb chicken ... but not venison ... but sure enough ... this has lamb in it too ! ! ! ! ! ! < br / > why ? ? ? why ca n't they just stick to the one protein they 're advertising ? ? ? peeves me off ! ! !\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "my dog \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "very frustrating ! ! ! \n",
      "\n",
      "\n",
      "Iter 1100, Cost= 0.849, Acc = 24.03%\n",
      "Iter 1200, Cost= 0.856, Acc = 23.40%\n",
      "Iter 1300, Cost= 0.795, Acc = 23.49%\n",
      "Iter 1400, Cost= 0.571, Acc = 29.91%\n",
      "Iter 1500, Cost= 0.670, Acc = 26.83%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "the bite size <UNK> are wonderful - 2 or 3 every few hours lets yu diet and never feel hungry .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "great dog \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "great \n",
      "\n",
      "\n",
      "Iter 1600, Cost= 0.572, Acc = 32.77%\n",
      "Iter 1700, Cost= 0.677, Acc = 29.37%\n",
      "Iter 1800, Cost= 0.867, Acc = 23.84%\n",
      "Iter 1900, Cost= 0.729, Acc = 23.02%\n",
      "Iter 2000, Cost= 0.729, Acc = 26.03%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "heard of them through a family member . my order was shipped and received in less than a week after it was placed . fast , efficient and will continue to use and recommend them to other people who miss our pr coffee ! thanks !\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "great \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "excellent service ! \n",
      "\n",
      "\n",
      "Iter 2100, Cost= 0.653, Acc = 25.60%\n",
      "Iter 2200, Cost= 0.779, Acc = 26.09%\n",
      "Iter 2300, Cost= 0.928, Acc = 22.56%\n",
      "Iter 2400, Cost= 0.644, Acc = 27.87%\n",
      "Iter 2500, Cost= 0.971, Acc = 21.66%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "my dog has food allergies so it 's hard to find snacks for her since she ca n't have biscuits or meaty treats . she loves these ! ! the are dried so they can be a little tough to chew so they might not be great for older dogs , but my yellow lab loves them ( she 's about 4 years old ) . these treats are highly recommended . she eats one almost every day as her night time snack .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "great dog ! ! \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "my dog loves these \n",
      "\n",
      "\n",
      "Iter 2600, Cost= 0.772, Acc = 27.52%\n",
      "Iter 2700, Cost= 0.710, Acc = 28.24%\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "predicted_summaries  = []\n",
    "actual_summaries = []\n",
    "\n",
    "with tf.Session() as sess:  # Start Tensorflow Session\n",
    "    display_step = 100\n",
    "    patience = 5\n",
    "\n",
    "    load = input(\"\\nLoad checkpoint? y/n: \")\n",
    "    print(\"\")\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    if load.lower() == 'y':\n",
    "\n",
    "        print('Loading pre-trained weights for the model...')\n",
    "\n",
    "        saver.restore(sess, 'Model_Backup/Seq2seq_summarization.ckpt')\n",
    "        sess.run(tf.global_variables())\n",
    "        sess.run(tf.tables_initializer())\n",
    "\n",
    "        with open('Model_Backup/Seq2seq_summarization.pkl', 'rb') as fp:\n",
    "            train_data = pickle.load(fp)\n",
    "\n",
    "        covered_epochs = train_data['covered_epochs']\n",
    "        best_loss = train_data['best_loss']\n",
    "        impatience = 0\n",
    "        \n",
    "        print('\\nRESTORATION COMPLETE\\n')\n",
    "\n",
    "    else:\n",
    "        best_loss = 2**30\n",
    "        impatience = 0\n",
    "        covered_epochs = 0\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        sess.run(tf.tables_initializer())\n",
    "\n",
    "    epoch=0\n",
    "    while (epoch+covered_epochs)<epochs:\n",
    "        \n",
    "        print(\"\\n\\nSTARTING TRAINING\\n\\n\")\n",
    "        \n",
    "        batches_indices = [i for i in range(0, len(train_batches_text))]\n",
    "        random.shuffle(batches_indices)\n",
    "\n",
    "        total_train_acc = 0\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for i in range(0, len(train_batches_text)):\n",
    "            \n",
    "            j = int(batches_indices[i])\n",
    "\n",
    "            cost,prediction,\\\n",
    "                acc, _ = sess.run([cross_entropy,\n",
    "                                   outputs,\n",
    "                                   accuracy,\n",
    "                                   train_op],\n",
    "                                  feed_dict={tf_text: train_batches_text[j],\n",
    "                                             tf_embd: embd,\n",
    "                                             tf_summary: train_batches_summary[j],\n",
    "                                             tf_true_summary_len: train_batches_true_summary_len[j],\n",
    "                                             tf_train: True})\n",
    "            \n",
    "            total_train_acc += acc\n",
    "            total_train_loss += cost\n",
    "\n",
    "            if i % display_step == 0:\n",
    "                print(\"Iter \"+str(i)+\", Cost= \" +\n",
    "                      \"{:.3f}\".format(cost)+\", Acc = \" +\n",
    "                      \"{:.2f}%\".format(acc*100))\n",
    "            \n",
    "            if i % 500 == 0:\n",
    "                \n",
    "                idx = random.randint(0,len(train_batches_text[j])-1)\n",
    "                \n",
    "                \n",
    "                \n",
    "                text = \" \".join([idx2vocab.get(vec,\"<UNK>\") for vec in train_batches_text[j][idx]])\n",
    "                predicted_summary = [idx2vocab.get(vec,\"<UNK>\") for vec in prediction[idx]]\n",
    "                actual_summary = [idx2vocab.get(vec,\"<UNK>\") for vec in train_batches_summary[j][idx]]\n",
    "                \n",
    "                print(\"\\nSample Text\\n\")\n",
    "                print(text)\n",
    "                print(\"\\nSample Predicted Summary\\n\")\n",
    "                pred = \"\"\n",
    "                for word in predicted_summary:\n",
    "                    if word == '<EOS>':\n",
    "                        break\n",
    "                    else:\n",
    "                        pred += word + \" \"\n",
    "                        print(word,end=\" \")\n",
    "                predicted_summaries.append(pred)\n",
    "                print(\"\\n\\nSample Actual Summary\\n\")\n",
    "                \n",
    "                actual = \"\"\n",
    "                for word in actual_summary:\n",
    "                    if word == '<EOS>':\n",
    "                        break\n",
    "                    else:\n",
    "                        actual += word + \" \"\n",
    "                        print(word,end=\" \")\n",
    "                print(\"\\n\\n\")\n",
    "                \n",
    "                actual_summaries.append(actual)\n",
    "                \n",
    "        print(\"\\n\\nSTARTING VALIDATION\\n\\n\")\n",
    "                \n",
    "        total_val_loss=0\n",
    "        total_val_acc=0\n",
    "                \n",
    "        for i in range(0, len(val_batches_text)):\n",
    "            \n",
    "            if i%100==0:\n",
    "                print(\"Validating data # {}\".format(i))\n",
    "\n",
    "            cost, prediction,\\\n",
    "                acc = sess.run([cross_entropy,\n",
    "                                outputs,\n",
    "                                accuracy],\n",
    "                                  feed_dict={tf_text: val_batches_text[i],\n",
    "                                             tf_embd: embd,\n",
    "                                             tf_summary: val_batches_summary[i],\n",
    "                                             tf_true_summary_len: val_batches_true_summary_len[i],\n",
    "                                             tf_train: False})\n",
    "            \n",
    "            total_val_loss += cost\n",
    "            total_val_acc += acc\n",
    "            \n",
    "        avg_val_loss = total_val_loss/len(val_batches_text)\n",
    "        \n",
    "        print(\"\\n\\nEpoch: {}\\n\\n\".format(epoch+covered_epochs))\n",
    "        print(\"Average Training Loss: {:.3f}\".format(total_train_loss/len(train_batches_text)))\n",
    "        print(\"Average Training Accuracy: {:.2f}\".format(100*total_train_acc/len(train_batches_text)))\n",
    "        print(\"Average Validation Loss: {:.3f}\".format(avg_val_loss))\n",
    "        print(\"Average Validation Accuracy: {:.2f}\".format(100*total_val_acc/len(val_batches_text)))\n",
    "              \n",
    "        if (avg_val_loss < best_loss):\n",
    "            best_loss = avg_val_loss\n",
    "            save_data={'best_loss':best_loss,'covered_epochs':covered_epochs+epoch+1}\n",
    "            impatience=0\n",
    "            with open('Model_Backup/Seq2seq_summarization.pkl', 'wb') as fp:\n",
    "                pickle.dump(save_data, fp)\n",
    "            saver.save(sess, 'Model_Backup/Seq2seq_summarization.ckpt')\n",
    "            print(\"\\nModel saved\\n\")\n",
    "              \n",
    "        else:\n",
    "            impatience+=1\n",
    "              \n",
    "        if impatience > patience:\n",
    "              break\n",
    "              \n",
    "              \n",
    "        epoch+=1\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Works\n",
    "\n",
    "* Beam Search\n",
    "* Pointer Mechanisms\n",
    "* BLEU\\ROUGE evaluation\n",
    "* Implement Testing\n",
    "* Complete Training and Optimize Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import rouge\n",
    "\n",
    "# Run ROUGE evaluation metric\n",
    "print(rouge(predicted_summaries, actual_summaries))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
